// ChunkCompressor.compute
// GPU chunk compression shader (LZ4-style, parallel per block column)
// Thread group: [numthreads(32,32,1)]

// Input: 32x32x32 blocks, 4 bytes each (int)
// Output: Compressed data (byte stream), metadata (compressed size)

#pragma kernel CSComputeSizes
#pragma kernel CSWritePayloads

StructuredBuffer<int>    ChunkInput; // [batch * 32*32*32] input blocks
RWByteAddressBuffer      ChunkCompressedOutput; // per-chunk output (unused in sizes pass)
RWStructuredBuffer<uint> ChunkMetadata; // per-chunk [payloadSize, originalSize]

// For write pass
StructuredBuffer<uint>   ChunkBaseBytes; // byte base offset for each chunk in the compact buffer
RWByteAddressBuffer      GlobalCompressedOutput; // compacted output buffer (all chunks concatenated)

// Constants
#define CHUNK_SIZE_X 32
#define CHUNK_SIZE_Y 32
#define CHUNK_SIZE_Z 32
#define BLOCKS_PER_CHUNK (CHUNK_SIZE_X*CHUNK_SIZE_Y*CHUNK_SIZE_Z)
#define COLUMN_COUNT (CHUNK_SIZE_X*CHUNK_SIZE_Y)

// Per-column shared arrays: local byte counts and final offsets (in column order)
groupshared uint sharedLocalBytes[COLUMN_COUNT];
groupshared uint sharedOffsets[COLUMN_COUNT];

// Compute only sizes: do the per-column compression locally and sum sizes to write payloadSize into metadata
[numthreads(32,32,1)]
void CSComputeSizes(uint3 groupThread : SV_GroupThreadID, uint3 groupId : SV_GroupID) {
    uint chunkIndex = groupId.x;
    uint lx = groupThread.x;
    uint ly = groupThread.y;
    bool inBounds = (lx < CHUNK_SIZE_X && ly < CHUNK_SIZE_Y);
    uint columnOffset = lx + ly * CHUNK_SIZE_X;
    uint inputBase = chunkIndex * BLOCKS_PER_CHUNK + columnOffset * CHUNK_SIZE_Z;
    uint localBytes = 0;

    // simple RLE per column
    if(inBounds) {
        int prev = ChunkInput[inputBase];
        uint runLength = 1;
        for(uint z = 1; z < CHUNK_SIZE_Z; ++z) {
            int curr = ChunkInput[inputBase + z];
            if(curr == prev && runLength < 0xFFFFFFFF) {
                runLength++;
            } else {
                localBytes += 8; // 4 bytes value + 4 bytes run length
                prev = curr;
                runLength = 1;
            }
        }
        localBytes += 8; // Final run
    }

    if(inBounds) sharedLocalBytes[columnOffset] = localBytes; else sharedLocalBytes[columnOffset] = 0;
    GroupMemoryBarrierWithGroupSync();

    if(lx == 0 && ly == 0) {
        uint running = 0;
        for(uint colIdx = 0; colIdx < COLUMN_COUNT; ++colIdx) { // Renamed from 'col' to 'colIdx'
            uint v = sharedLocalBytes[colIdx];
            sharedOffsets[colIdx] = running;
            running += v;
        }
        // write payload size and original size into metadata
        ChunkMetadata[chunkIndex * 2 + 0] = running;
        ChunkMetadata[chunkIndex * 2 + 1] = BLOCKS_PER_CHUNK * 4;
    }
}

// Second pass: write compacted output into GlobalCompressedOutput using ChunkBaseBytes[chunkIndex] as byte base
[numthreads(32,32,1)]
void CSWritePayloads(uint3 groupThread : SV_GroupThreadID, uint3 groupId : SV_GroupID) {
    uint chunkIndex = groupId.x;
    uint lx = groupThread.x;
    uint ly = groupThread.y;
    bool inBounds = (lx < CHUNK_SIZE_X && ly < CHUNK_SIZE_Y);
    uint columnOffset = lx + ly * CHUNK_SIZE_X;
    uint inputBase = chunkIndex * BLOCKS_PER_CHUNK + columnOffset * CHUNK_SIZE_Z;

    // Process compression and write directly to output to avoid large local arrays
    uint localBytes = 0u;

    if(inBounds) {
        int prev = ChunkInput[inputBase];
        uint runLength = 1;

        for(uint z = 1; z < CHUNK_SIZE_Z; ++z) {
            int curr = ChunkInput[inputBase + z];
            if(curr == prev && runLength < 0xFFFFFFFF) {
                runLength++;
            } else {
                localBytes += 8;
                prev = curr;
                runLength = 1;
            }
        }
        localBytes += 8; // Final run
    }

    // store local byte counts
    if(inBounds) sharedLocalBytes[columnOffset] = localBytes; else sharedLocalBytes[columnOffset] = 0;
    GroupMemoryBarrierWithGroupSync();

    // compute per-column offsets (single thread per group)
    if(lx == 0 && ly == 0) {
        uint running = 0;
        for(uint colIdx = 0; colIdx < COLUMN_COUNT; ++colIdx) { // Renamed from 'col' to 'colIdx'
            uint v = sharedLocalBytes[colIdx];
            sharedOffsets[colIdx] = running;
            running += v;
        }
        // write offset table starting at chunkBase
        uint chunkBase = ChunkBaseBytes[chunkIndex];
        for(uint colIdx2 = 0; colIdx2 < COLUMN_COUNT; ++colIdx2) { // Renamed from 'col' to 'colIdx2'
            GlobalCompressedOutput.Store(chunkBase + colIdx2 * 4, sharedOffsets[colIdx2]);
        }
    }
    GroupMemoryBarrierWithGroupSync();

    // Now write the actual compressed data directly to output
    if(inBounds) {
        uint chunkBase = ChunkBaseBytes[chunkIndex];
        uint dataStart = chunkBase + COLUMN_COUNT * 4;
        uint outPtr = dataStart + sharedOffsets[columnOffset];

        // Recompress and write directly to avoid local storage
        int prev = ChunkInput[inputBase];
        uint runLength = 1;

        for(uint z = 1; z < CHUNK_SIZE_Z; ++z) {
            int curr = ChunkInput[inputBase + z];
            if(curr == prev && runLength < 0xFFFFFFFF) {
                runLength++;
            } else {
                // Write value,run pair directly to global output
                GlobalCompressedOutput.Store(outPtr, asuint(prev));
                GlobalCompressedOutput.Store(outPtr + 4, runLength);
                outPtr += 8;
                prev = curr;
                runLength = 1;
            }
        }
        // Write final run
        GlobalCompressedOutput.Store(outPtr, asuint(prev));
        GlobalCompressedOutput.Store(outPtr + 4, runLength);
    }
    GroupMemoryBarrierWithGroupSync();
}
